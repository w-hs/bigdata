\section{Realisierung}

Die Implementierung erfolgt iterativ, dass heißt wir entwickeln unterschiedliche gegebenenfalls aufeinander aufbauende Lösungen
und versuchen, uns mit jedem Versuch zu verbessern. Unseren Fortschritt messen wir, indem wir in jeder Iteration ein einreichbares Ergebnis erzeugen. Die Bewertung erfolgt somit über einen internen Algorithmus von Kaggle, welcher einen Platz in der Wettbewerbsrangliste und einen Score berechnet.

Das Vorgehen bei der Realisierung umfasst das Testen und Entwickeln der Skripte lokal auf 
dem eigenen Rechner ggf. unter Verwendung einer SQL-Datenbank. 
Anschließend werden darauf basierend Hive-Skripte erstellt und in einem Hadoop-Cluster
im AWS ausgeführt.

\subsection{Implementierung}
Im Nachfolgenden werden alle Iterationen der Reihe nach vorgestellt. Die Ergebnisse, 
die wir nach jeder Iteration bei Kaggle eingereicht haben, sind im
Github-Repository\footnote{\url{https://github.com/w-hs/bigdata/tree/master/submissions}}
abgelegt und können dort zur nachträglichen Überprüfung eingesehen werden.

\subsubsection{Iteration 0}
\label{sec:iteration0}

Als Erstes wollen wir eine Grundlage zur Bewertung folgender Versuche schaffen und das Einreichen einer
Lösung bei Kaggle üben. Dazu erzeugen wir eine Datei, die jedem Kunden die Kaufwahrscheinlichkeit 0
zuordnet. Hierzu verwenden wir eine rudimentäre SQL-Abfrage:

\begin{lstlisting}[language=SQL]
SELECT DISTINCT(h.id), 0.0 AS repeatProbability 
FROM test_history h
\end{lstlisting}

Die Bewertung von Kaggle resultierte in folgender Platzierung:

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 932 & 0.50000  \\ 
	\hline 
\end{tabular}

\subsubsection{Iteration 1}

Im nächsten Schritt wollen wir den ersten "`Prior Category Benchmark"'\footnote{Die Benchmarks befinden sich auf dem Leaderboard unter \\
\url{https://www.kaggle.com/c/acquire-valued-shoppers-challenge/leaderboard}} von Kaggle implementieren.
Dieser Benchmark ordnet Kunden, die bereits ein Produkt der Gutscheinkategorie gekauft haben, 
die Wiederkaufwahrscheinlichkeit 1 zu. Allen anderen Kunden wird die Wahrscheinlichkeit 0 zugeordnet.

Wir bestimmen die Kunden mit einer Wahrscheinlichkeit von 1 über folgende SQL-Abfrage:
\begin{lstlisting}[language=SQL]
SELECT DISTINCT h.id, 1.0 AS repeatProbability
FROM test_history h 
INNER JOIN offers o ON (h.offer = o.offer)
INNER JOIN transactions t ON (t.id = h.id)
WHERE t.category = o.category
\end{lstlisting}

Um das Ergebnis bei Kaggle einzureichen, fehlen noch die Kunden mit einer Wahrscheinlichkeit von 0.
Um diese hinzuzufügen wurde ein Python-Skript entwickelt, das eine unvollständige Einreichung
um die fehlenden Einträge erweitert. Für die Kunden ohne Einträge wird eine Wahrscheinlichkeit von 0 eingetragen.
Dieses Skript (s. Anhang \ref{code:complete-submission}) kann in weiteren Iterationen verwendet werden,
um sicher zu stellen, dass das Ergebnis vollständig ist und die Überprüfung von Kaggle übersteht. 

Die Bewertung von Kaggle ergibt wie erwartet:

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 747 & 0.52000  \\ 
	\hline 
\end{tabular}

\subsubsection{Iteration 2}

In Iteration 1 haben wir den Benchmark mit der niedrigsten Bewertung implementiert. Jetzt wollen wir 
den besten Benchmark umsetzen. Der "`Prior (Brand \& Company \& Category) Benchmark"' ist sehr ähnlich
zu dem ersten Benchmark. Wir ordnen jedem Kunden, der schon einmal ein Produkt der Gutscheinmarke,
des Gutscheinunternehmens und der Gutscheinkategorie gekauft hat, eine 1 zu. 
Das wird über folgendes SQL-Skript erreicht:

\begin{lstlisting}[language=SQL]
SELECT DISTINCT h.id, 1.0 AS repeatProbability
FROM test_history h 
INNER JOIN offers o ON (h.offer = o.offer)
INNER JOIN transactions t ON (t.id = h.id)
WHERE t.category = o.category
  AND t.company = o.company
  AND t.brand = o.brand
\end{lstlisting}

Bevor wir das Ergebnis einreichen, verwenden wir das Skript zum Vervollständigen aus Iteration 1, welches allen anderen Kunden eine 0 zuweist.

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 627 & 0.56425  \\ 
	\hline 
\end{tabular}

Mit dieser Änderung konnten wir unsere Platzierung erneut spürbar verbessern.

\subsubsection{Iteration 3}

Nachdem wir in den vorherigen Iterationen die vorgegebenen Benchmarks von Kaggle implementiert haben,
wollen wir jetzt ein richtiges Data-Mining-Verfahren verwenden. Dazu ermitteln wir im ersten Schritt
auf Basis der Transaktionsdaten Features für jeden Kunden. Bei der Wahl der Features orientieren
wir uns an den Ergebnissen von einem 
Kaggle-Nutzer\footnote{\url{http://mlwave.com/predicting-repeat-buyers-vowpal-wabbit/}}.

Folgende Merkmale werden jeweils für die Marke (brand), das Unternehmen (company) und die Kategorie (category)
des Gutscheins (offer) ermittelt:
\begin{itemize}
	\item Für wie viele Produkte mit welchem Preis gilt der Gutschein?
	\item Wie oft hat der Kunde in den letzen 30, 60, 90 und 180 Tagen gekauft? (Transaktionszahl)
	\item Wie viel hat der Kunde in den letzen 30, 60, 90 und 180 Tagen gekauft? (Anzahl gekaufter Produkte)
	\item Für wie viel Geld hat der Kunde in den letzen 30, 60, 90 und 180 Tagen gekauft? (Kosten)
\end{itemize}

Diese Features werden über eine Reihe von SQL-Abfragen erstellt. Um die Zwischenergebnisse auch für
weitere Iterationen nutzen zu können, legen wir Tabellen mit den Ergebnissen an. Das erlaubt zusätzlich
einen strukturierten Aufbau der Merkmale.

Da die Kunden-Daten verteilt über die \texttt{train\_history} und die \texttt{test\_history} sind, 
bilden wir zuerst die Vereinigung der Kunden-Daten. Hier kann "`UNION ALL"' verwendet werden, da die 
Mengen der Kunden-IDs in den Trainings- und Testdaten disjunkt sind.

\begin{lstlisting}[language=SQL]
CREATE TABLE customers AS 
  SELECT te.id, te.offer, te.offerdate
    FROM test_history te
  UNION ALL
  SELECT tr.id, tr.offer, tr.offerdate
    FROM train_history tr 
\end{lstlisting}

Danach ermitteln wir die Transaktionsdaten, die sich jeweils auf die Marke, das Unternehmen oder 
die Kategorie des Gutscheins für den Kunden beziehen. Dabei bestimmen wir zusätzlich, wie viele
Tage vor dem Gutschein die Transaktion durchgeführt wurde. Beispielhaft ist hier die Abfrage
für die Marke dargestellt. Die anderen Anfragen werden analog erstellt und sind im Anhang
\ref{sql:filtered_tx} hinterlegt.
\begin{lstlisting}[language=SQL]
CREATE TABLE filtered_tx_brand AS
  SELECT t.id, t.purchasequantity, t.purchaseamount, 
         (c.offerdate - t.date) AS daysbefore 
  FROM transactions t 
  INNER JOIN customers c ON (t.id = c.id)
  INNER JOIN offers o ON (c.offer = o.offer)
  WHERE o.brand = t.brand
\end{lstlisting}

Anschließend werden die gefilterten Transaktionsdaten gruppiert nach
Zeitintervallen ausgewertet. Die vollständigen Abfragen befinden sich
in Anhang \ref{sql:features_bcc}. Hier wird ein Auszug aus der
Abfrage für die Marken-Features dargestellt:
\begin{lstlisting}[language=SQL]
CREATE TABLE category_features AS
SELECT ever.id, ever.tx_count, ever.quantity, ever.cost,
  before30.tx_count AS tx_count_30,
  before30.quantity AS quantity_30,
  before30.cost AS cost_30,
  before60.tx_count AS tx_count_60,
  before60.quantity AS quantity_60,
  before60.cost AS cost_60,
  before90.tx_count AS tx_count_90,
  before90.quantity AS quantity_90,
  before90.cost AS cost_90,
  before180.tx_count AS tx_count_180,
  before180.quantity AS quantity_180,
  before180.cost AS cost_180
FROM 
(SELECT id, COUNT(*) AS tx_count, 
  SUM(purchasequantity) AS quantity, 
  SUM(purchaseamount) AS cost
  FROM filtered_tx_category
  GROUP BY id
) ever
LEFT JOIN
(SELECT id, COUNT(*) AS tx_count, 
  SUM(purchasequantity) AS quantity, 
  SUM(purchaseamount) AS cost
  FROM filtered_tx_category
  WHERE daysbefore <= 30
  GROUP BY id
) before30
ON (ever.id = before30.id)
LEFT JOIN
...
\end{lstlisting}

Jetzt müssen die unterschiedlichen Features nur noch zusammengeführt werden. Dies passiert
in Anhang \ref{sql:features_combined_3} und umfasst einen Join zwischen den
unterschiedlichen Features auf Basis der Kunden-ID.

Um das Data-Mining-Tool Vowpal Wabbit verwenden zu können, müssen die Features in ein
passendes Format gewandelt werden. Das Ergebnis der Abfrage für die Features kann
als CSV abgespeichert werden. Die Umwandlung in das Format von Vowpal Wabbit
erfolgt über ein Python-Skript (s. Anhang \ref{code:features-to-vw}).

Anschließend erstellen wir ein Model auf Basis der Trainingsdaten und nutzen dieses
Model zur Voraussage der Wiederkaufwahrscheinlichkeiten für die Kunden aus den Testdaten.
\begin{lstlisting}
vw train.vw -c -k --passes 40 -l 0.85 -f model.vw --loss_function quantile --quantile_tau 0.6
vw test.vw -t -i model.vw -p shop.preds.txt
\end{lstlisting}

Das Ergebnis von Vowpal Wabbit muss anschließend in ein entsprechendes Format zur Einreichung bei 
Kaggle konvertiert werden. Dies geschieht über das Python-Skript im Anhang 
\ref{code:vw-preds-to-csv}. 

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 499 & 0.58051  \\ 
	\hline 
\end{tabular}

Im Ergebnis können wir uns mit dieser Einreichung
schon etwas von den zuvor durchgeführten Benchmarks absetzen.

\subsubsection{Iteration 4}
Bei den Features, die wir bisher betrachtet haben, gehen wir von einer
positiven Wirkung auf das Kaufverhalten aus. Wir ergänzen diese um negative Features.

Bei der Umwandlung der Features von CSV in das Format von Vowpal Wabbit 
erzeugen wir Features für Kunden, die noch nie Produkte der Marke,
des Unternehmens bzw. der Kategorie ihres Gutscheins gekauft haben.
Dazu wird das zuständige Python-Skript erweitert (die angepasste Version ist in Anhang 
\ref{code:features-to-vw-4}).
\begin{lstlisting}[language=Python]
# Negative Features erzeugen, wenn nichts gekauft wurde
if len(row['b_tx_count']) == 0:
	output += ' b_never:1'
if len(row['co_tx_count']) == 0:
	output += ' co_never:1'
if len(row['ca_tx_count']) == 0:
	output += ' ca_never:1'
\end{lstlisting}

In dieser Iteration wird das Ergebnis aus Iteration 3 geringfügig verbessert.

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 344 & 0.58786  \\ 
	\hline 
\end{tabular}

Das Ergebnis liegt über dem besten des letzten Jahrgangs, welche den 452. Platz mit einem Score von 0.58519 belegen.

\subsubsection{Iteration 5}

Jetzt erweitern wir die Features um Kombinationen. Wir führen neue Features ein, die den Wert 1 haben,
falls der Kunde von Marke und Unternehmen, Marke und Kategorie oder allen drei Eigenschaften bereits
gekauft hat.

\begin{lstlisting}[language=SQL]
SELECT f.*, 
 CAST(b_tx_count > 0 AND co_tx_count > 0 AND ca_tx_count > 0 
  AS INTEGER) AS any_b_co_ca,
 CAST(b_tx_count > 0 AND ca_tx_count > 0 AS INTEGER) AS any_b_ca,
 CAST(b_tx_count > 0 AND co_tx_count > 0 AS INTEGER) AS any_b_co
FROM test_features f

SELECT f.*, 
 CAST(b_tx_count > 0 AND co_tx_count > 0 AND ca_tx_count > 0 
  AS INTEGER) AS any_b_co_ca,
 CAST(b_tx_count > 0 AND ca_tx_count > 0 AS INTEGER) AS any_b_ca,
 CAST(b_tx_count > 0 AND co_tx_count > 0 AS INTEGER) AS any_b_co
FROM train_features f
\end{lstlisting}

Wir verwenden wie zuvor Vowpal Wabbit, um die Vorhersage zu generieren:

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 241 & 0.59355  \\ 
	\hline 
\end{tabular}

Mit diesem Verfahren erreichen wir erneut eine Verbesserung unserer Platzierung.

\subsubsection{Iteration 6}
Bisher haben wir nur einen kleinen Teil der Daten zur Generierung von Features verwendet,
da wir nur Transaktionen beachtet haben, die sich auf den entsprechenden Gutschein für 
den Kunden beziehen. 
Jetzt führen wir die globalen Features "`Anzahl der Transaktionen"' und "`Gesamte Umsatzsumme"' ein. Diese Features erzeugen wir in einer eigenen Tabelle und führen diese mit den bereits generierten Features zusammen (s. Anhang \ref{code:totals}).

\begin{lstlisting}[language=SQL]
CREATE TABLE totals AS
  SELECT t.id, COUNT(*) AS total_count, 
         SUM(t.purchaseamount) AS total_spent
  FROM transactions t
  GROUP BY t.id
\end{lstlisting}

Das Ergebnis fällt enttäuschend aus und liegt leicht unter der bisherigen Bestmarke:

\begin{tabular}{|c|c|}
	\hline \textbf{Platzierung} & \textbf{Bewertung} \\ 
	\hline 262 & 0.59228  \\ 
	\hline 
\end{tabular}

\subsection{Amazon-Web-Services}
Im Laufe des Projekts war es für uns von großem Vorteil, lokal mit den Daten zu arbeiten und SQL-Skripte lokal entwickeln zu können. Auf diese Weise haben wir sowohl die Kosten, die bei der Benutzung von AWS entstanden wären, eingespart als auch unseren Zeitaufwand minimiert. Die Ausführung der Skripte dauert in der Cloud um ein Vielfaches länger als bei einer lokalen Nutzung. Neben der eigentlichen Ausführung der Skripte im AWS ist es nötig, dass Rechencluster instantiiert, Hadoop und Hive installiert und der Cluster nach Abschluss des Jobs terminiert wird. Dieser Aufwand rechnet sich erst bei entsprechend großen Datenmengen, sodass die Laufzeit der Skripte überwiegt. Unsere Dateigröße von rund 20 GB ist noch nicht im Bereich Big-Data anzuordnen. Entsprechend lohnt auch ein Einsatz von Werkzeugen wie Hadoop in AWS nicht.

Diese Erfahrung haben wir bei der Benutzung von AWS gesammelt. Unser Vorgang umfasst dabei immer folgende Abläufe:
\begin{enumerate}
\item Cluster erstellen und konfigurieren
\subitem Servergröße auswählen
\subitem Zu installierende Module - Hive
\subitem Logging und Auto-Terminierung einstellen
\item Jobs konfigurieren
\subitem Anlegen der Tabellen auf Basis der Kaggle-Daten im S3
\subitem Ausführen unserer Analysen und Speichern der Ergebnisse im S3
\end{enumerate}

\subsubsection{Anlegen der Tabellen}
Die von Kaggle bereitgestellten Daten befinden sich im CSV-Format im S3.
Zur Erstellung der Hive-Datenbank nutzen wir Skripte, die Tabellen auf Basis der Dateien im S3 anlegen. Im Folgenden stellen wir exemplarisch ein Skript zur Erstellung der Tabelle "`offers"' vor. 
\\
\begin{lstlisting}[style=hive]
CREATE EXTERNAL TABLE offers (
   offer BIGINT, category BIGINT, 
   quantity BIGINT, company BIGINT, 
   offervalue DOUBLE, brand BIGINT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION 's3://de.whs.fdt.kaggle.acquire-valued-shoppers- challenge/input_data_orig/offers/';
\end{lstlisting}

Nachfolgend werden die Hive-spezifische Schlüsselwörter beschrieben:
\begin{description}
\item[EXTERNAL] Erlaubt die Tabelle auf Basis einer externen Datei anzulegen.
\item[ROW FORMAT] Gibt das Trennzeichen zwischen Spaltenwerten an.
\item[LINES] Gibt das Zeichen zur Markierung des Zeilenende an.
\item[STORED] Gibt das Format der externen Datei an.
\item[LOCATION] Gibt den externen Speicherort an.
\end{description}

\subsubsection{Anfragen ausführen}
Anhand des SQL-Skripts aus der ersten Iteration wollen wir an dieser Stelle exemplarisch die Erstellung der entsprechenden Hive-Skripte erläutern.

\begin{lstlisting}[style=hive]
INSERT OVERWRITE DIRECTORY
    's3://de.whs.fdt.kaggle.acquire-v../output_data/iteration0/'
  SELECT DISTINCT (h.id), 0.0 AS repeatProbability
  FROM test_history h;
\end{lstlisting}

Der Unterschied zu dem bekannten SQL-Skript (s. Abschnitt \ref{sec:iteration0}) liegt in der neu hinzugekommen ersten Zeile. Der entsprechende Befehl schreibt das Ergebnis der darauffolgenden Abfrage in das angegebene Verzeichnis, welches im S3 in unserem Bucket liegt. Analog zu diesem Beispiel haben wir ebenfalls die Hive-Skripte für alle weiteren Iterationen umgesetzt.

  

